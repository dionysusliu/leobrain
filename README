Sure ‚Äî here‚Äôs a condensed, production-style README.md that captures everything above (MVP plan + architecture + tech stack + iteration workflow) in a single document.
You can drop it directly into your repository root as README.md.

‚∏ª


# üåê LEOBRAIN ‚Äî Personal Web Intelligence & ML Pipeline (MVP)

> A personal "external brain" that crawls multi-source web data (news, finance, forums),  
> analyzes trends & sentiment via ML plugins, and visualizes insights on a dashboard.

---

## üöÄ Goal
1. Collect public data across diverse sources (finance, tech, knowledge forums).
2. Run ML pipelines ‚Äî sentiment, keyword, topic, correlation, trend.
3. Persist and visualize insights for personal research & investment reasoning.
4. Evolve into a modular, cloud-ready intelligence platform.

---

## üß© MVP Scope (Phase 1)
| Area | Goal |
|------|------|
| **Data Ingestion** | Crawl 2‚Äì3 sources hourly, respect robots.txt, retry/back-off. |
| **Storage** | Save metadata + body ‚Üí Postgres + MinIO (S3-compatible). |
| **Analysis** | Sentiment + Keyword plugins ‚Üí store results (JSONB). |
| **Dashboard** | Web UI filtering (time/source/keyword) + sentiment & keyword charts + CSV export + daily email. |
| **Scheduler** | Prefect flows or Airflow DAGs with retry/log view. |
| **Observability** | Prometheus metrics + structured logs + Grafana panel. |

---

## üèóÔ∏è Architecture

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Crawlers   ‚îÇ‚îÄ‚îÄ‚ñ∫ ‚îÇ  Pipeline  ‚îÇ‚îÄ‚îÄ‚ñ∫ ‚îÇ Analysis Plugins   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ                ‚îÇ                     ‚îÇ
‚ñº                ‚ñº                     ‚ñº
MinIO (S3)       PostgreSQL           Prefect Scheduler
‚îÇ                ‚îÇ                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ FastAPI API ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ
Next.js UI
‚îÇ
Grafana + Prometheus

---

## ‚öôÔ∏è Core Tech Stack
| Layer | Tech |
|-------|------|
| **Backend API** | Python 3.11 + FastAPI + SQLModel/Pydantic |
| **Frontend** | Next.js (React 18) + Tailwind + Chart.js |
| **DB** | PostgreSQL 15 |
| **Object Store** | MinIO (local) ‚Üí S3/OSS/GCS (later) |
| **Queue / Cache** | Redis + Celery or Prefect Tasks |
| **Search (Optional P1)** | OpenSearch or Elastic Cloud |
| **ML Libs** | spaCy / scikit-learn / HuggingFace Transformers |
| **Observability** | Prometheus + Grafana + OpenTelemetry SDK |
| **Infra** | Docker + docker-compose ‚Üí Helm + Terraform on K8s |
| **CI/CD** | GitHub Actions (test ‚Üí build ‚Üí push images) |

---

## üß† Plugin Architecture (v1)

Each analysis task is a plugin implementing a shared protocol.

```python
class AnalysisPlugin(Protocol):
    name: str
    version: str
    def run(self, item: Content) -> AnalysisResult: ...

	‚Ä¢	Built-in Plugins
	‚Ä¢	sentiment ‚Üí polarity ‚àà {-1, 0, 1}, score [0, 1]
	‚Ä¢	keywords ‚Üí [{token, weight}]
	‚Ä¢	Future
	‚Ä¢	topic, correlation, trend
	‚Ä¢	Enable/Disable via API or config.
	‚Ä¢	Results stored as JSONB in analysis_results.

‚∏ª

üóÑÔ∏è Data Model (Simplified)

Table	Fields
contents	id (pk), source, url, title, author, published_at, body_ref, lang
analysis_results	id, content_id ‚Üí fk, plugin, version, result_type, payload(jsonb), created_at
job_runs	id, job_name, status, started_at, ended_at, error_log


‚∏ª

üß∞ Local Setup

Requirements
	‚Ä¢	Docker + docker-compose
	‚Ä¢	Python 3.11 (optional local dev)
	‚Ä¢	Node 18+

Start All Services

git clone https://github.com/<you>/InsightCrawler
cd InsightCrawler
docker compose up -d

Services:
	‚Ä¢	FastAPI ‚Üí http://localhost:8000
	‚Ä¢	Next.js ‚Üí http://localhost:3000
	‚Ä¢	Prefect ‚Üí http://localhost:4200
	‚Ä¢	Grafana ‚Üí http://localhost:3001

‚∏ª

üß™ Verification Checklist (MVP)

Stage	Success Criteria
Ingestion	‚â• 50 new docs per hour, HTTP success rate > 95 %.
Analysis	Sentiment & keyword results for each doc, avg latency < 5 s.
Dashboard	Charts render < 2 s, export CSV OK, daily email sent.
Recovery	Task failure ‚Üí auto-retry & logged.
Metrics	/metrics exposes QPS, error rate, latency.


‚∏ª

üîÅ Iteration Workflow

Sprint	Deliverable
Week 1	Crawl 1 source ‚Üí store ‚Üí sentiment + keyword ‚Üí UI chart.
Week 2	Add 2 sources + Prefect panel + daily email + Prometheus.
Week 3	Topic plugin + OpenSearch + Helm/Terraform skeleton.

Principles:
	‚Ä¢	Vertical slice > big-bang.
	‚Ä¢	Each plugin = feature flag (off by default).
	‚Ä¢	Keep 1 click rollback (Alembic + Docker tag).

‚∏ª

‚òÅÔ∏è Cloud Migration Path

Stage	Environment
1. Single VM	Run docker-compose on EC2 / ECS / Aliyun ECS.
2. ÊâòÁÆ°‰∏â‰ª∂Â•ó	S3 + RDS (Postgres) + Elastic Cloud for search.
3. Kubernetes	Deploy via Helm on EKS/GKE/ACK + GitOps (Argo CD).
4. ML ÊâòÁÆ°	Move inference ‚Üí SageMaker / Vertex AI / PAI.
5. Micro Plugins	Heavy ML plugins ‚Üí gRPC microservices with HPA.


‚∏ª

üß© Repo Structure

repo/
  backend/
    app/           # FastAPI API
    plugins/       # analysis modules
    workers/       # celery/prefect tasks
    common/        # contracts/models
  frontend/        # Next.js dashboard
  deploy/
    docker-compose.yml
    helm/
    terraform/
  ops/
    grafana/
    prometheus/
  tests/
    e2e/
  README.md


‚∏ª

üß† Metrics to Watch

Category	Key Metric
Ingestion	New docs/hr, success %, avg fetch latency
Analysis	Plugin error %, avg processing latency
Output	Sentiment coverage %, keyword recall ‚â• N
System	CPU/RAM usage, DB connections, API P95 latency


‚∏ª

‚úÖ Next Steps
	1.	Implement the crawler + 2 plugins prototype.
	2.	Stand up docker-compose stack & verify end-to-end flow.
	3.	Add Prefect orchestration + Prometheus metrics.
	4.	Iterate weekly (add topic plugin ‚Üí search ‚Üí trend forecast).
	5.	Gradually containerize and migrate to cloud.

‚∏ª

## Quick Start

1. Clone and set up environment:
cd leobrain
cp backend/.env.example backend/.env2. Start infrastructure services:
cd deploy
docker compose up -d3. Install backend dependencies:ash
cd ../backend
pip install -r requirements.txt
python -m spacy download en_core_web_sm4. Initialize database:ash
alembic revision --autogenerate -m "Initial migration"
alembic upgrade head5. Start backend:h
uvicorn app.main:app --reload --host 0.0.0.0 --port 80006. Install frontend dependencies:
cd ../frontend
npm install7. Start frontend:
npm run devAccess:
- API: http://localhost:8000
- Frontend: http://localhost:3000
- Grafana: http://localhost:3001
- MinIO Console: http://localhost:9001


¬© 2025 InsightCrawler ‚Äî Personal Knowledge System Prototype.

---

Would you like me to generate the actual `docker-compose.yml` + folder skeleton referenced in this README next?