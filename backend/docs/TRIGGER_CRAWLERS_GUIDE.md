# è§¦å‘çˆ¬è™«ä»»åŠ¡æŒ‡å—

æœ¬æŒ‡å—è¯´æ˜å¦‚ä½•è§¦å‘å•ä¸ªæˆ–å¤šä¸ªçˆ¬è™«ä»»åŠ¡ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹å¼ 1: ä½¿ç”¨è„šæœ¬ï¼ˆæ¨èï¼‰

```bash
cd backend

# è§¦å‘æ‰€æœ‰ç«™ç‚¹
python scripts/trigger_crawlers.py

# è§¦å‘æŒ‡å®šç«™ç‚¹
python scripts/trigger_crawlers.py --sites bbc hackernews techcrunch

# å¹¶è¡Œè§¦å‘ï¼ˆæ›´å¿«ï¼‰
python scripts/trigger_crawlers.py --all --parallel
```

### æ–¹å¼ 2: ä½¿ç”¨ API

#### è§¦å‘å•ä¸ªç«™ç‚¹

```bash
curl -X POST http://localhost:8000/api/v1/crawlers/sites/bbc/crawl
```

#### æ‰¹é‡è§¦å‘å¤šä¸ªç«™ç‚¹

```bash
# è§¦å‘æŒ‡å®šç«™ç‚¹ï¼ˆä¸²è¡Œï¼‰
curl -X POST http://localhost:8000/api/v1/crawlers/sites/batch-crawl \
  -H "Content-Type: application/json" \
  -d '{
    "sites": ["bbc", "hackernews", "techcrunch"],
    "parallel": false
  }'

# è§¦å‘æ‰€æœ‰ç«™ç‚¹ï¼ˆå¹¶è¡Œï¼‰
curl -X POST http://localhost:8000/api/v1/crawlers/sites/batch-crawl \
  -H "Content-Type: application/json" \
  -d '{
    "sites": null,
    "parallel": true
  }'
```

### æ–¹å¼ 3: ä½¿ç”¨ API æ–‡æ¡£ï¼ˆSwagger UIï¼‰

1. è®¿é—® http://localhost:8000/docs
2. æ‰¾åˆ° `POST /api/v1/crawlers/sites/batch-crawl` ç«¯ç‚¹
3. ç‚¹å‡» "Try it out"
4. è¾“å…¥è¯·æ±‚ä½“ï¼Œä¾‹å¦‚ï¼š
   ```json
   {
     "sites": ["bbc", "hackernews"],
     "parallel": true
   }
   ```
5. ç‚¹å‡» "Execute"

## ğŸ“‹ å¯ç”¨ç«™ç‚¹

æŸ¥çœ‹æ‰€æœ‰é…ç½®çš„ç«™ç‚¹ï¼š

```bash
# ä½¿ç”¨è„šæœ¬
python scripts/trigger_crawlers.py --sites  # ä¼šæ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯

# ä½¿ç”¨ API
curl http://localhost:8000/api/v1/crawlers/sites
```

å¸¸è§ç«™ç‚¹åŒ…æ‹¬ï¼š
- **æ–°é—»ç±»**: `bbc`, `theguardian`, `npr`, `cnbc`, `wsj_cn`
- **è´¢ç»ç±»**: `yahoo_finance_news`, `yahoo_finance_top`, `financial_times`, `dowjones`, `thomsonreuters`, `xueqiu`
- **ç§‘æŠ€ç±»**: `hackernews`, `techcrunch`, `36kr`, `huxiu`

## ğŸ”§ è„šæœ¬ä½¿ç”¨è¯´æ˜

### åŸºæœ¬ç”¨æ³•

```bash
# è§¦å‘æ‰€æœ‰ç«™ç‚¹ï¼ˆä¸²è¡Œï¼‰
python scripts/trigger_crawlers.py

# è§¦å‘æ‰€æœ‰ç«™ç‚¹ï¼ˆå¹¶è¡Œï¼Œæ›´å¿«ï¼‰
python scripts/trigger_crawlers.py --all --parallel

# è§¦å‘æŒ‡å®šç«™ç‚¹
python scripts/trigger_crawlers.py --sites bbc hackernews techcrunch

# è§¦å‘æŒ‡å®šç«™ç‚¹ï¼ˆå¹¶è¡Œï¼‰
python scripts/trigger_crawlers.py --sites bbc hackernews --parallel
```

### å‚æ•°è¯´æ˜

- `--sites SITE1 SITE2 ...`: æŒ‡å®šè¦è§¦å‘çš„ç«™ç‚¹åˆ—è¡¨
- `--all`: è§¦å‘æ‰€æœ‰é…ç½®çš„ç«™ç‚¹
- `--parallel`: å¹¶è¡Œè§¦å‘æ‰€æœ‰ä»»åŠ¡ï¼ˆé»˜è®¤ä¸²è¡Œï¼‰
- `--api-url URL`: æŒ‡å®š API æœåŠ¡å™¨åœ°å€ï¼ˆé»˜è®¤: http://localhost:8000ï¼‰

### ç¤ºä¾‹

```bash
# ç¤ºä¾‹ 1: è§¦å‘æ‰€æœ‰æ–°é—»ç±»ç«™ç‚¹
python scripts/trigger_crawlers.py --sites bbc theguardian npr --parallel

# ç¤ºä¾‹ 2: è§¦å‘æ‰€æœ‰è´¢ç»ç±»ç«™ç‚¹
python scripts/trigger_crawlers.py --sites yahoo_finance_news financial_times dowjones --parallel

# ç¤ºä¾‹ 3: è§¦å‘æ‰€æœ‰ç§‘æŠ€ç±»ç«™ç‚¹
python scripts/trigger_crawlers.py --sites hackernews techcrunch 36kr huxiu --parallel

# ç¤ºä¾‹ 4: è§¦å‘æ‰€æœ‰ç«™ç‚¹ï¼ˆå¹¶è¡Œï¼Œæœ€å¿«ï¼‰
python scripts/trigger_crawlers.py --all --parallel
```

## ğŸ“Š æŸ¥çœ‹ä»»åŠ¡çŠ¶æ€

### åœ¨ Prefect WebUI ä¸­æŸ¥çœ‹

1. è®¿é—® http://localhost:4200
2. ç‚¹å‡»å·¦ä¾§å¯¼èˆªæ çš„ **"Flow Runs"**
3. æŸ¥çœ‹ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€ï¼š
   - **Running**: æ­£åœ¨æ‰§è¡Œ
   - **Completed**: å·²å®Œæˆ
   - **Failed**: å¤±è´¥
   - **Pending**: ç­‰å¾…æ‰§è¡Œ

### é€šè¿‡ API æŸ¥çœ‹

```bash
# æŸ¥çœ‹æ‰€æœ‰ç«™ç‚¹çŠ¶æ€
curl http://localhost:8000/api/v1/crawlers/sites

# æŸ¥çœ‹ç‰¹å®šç«™ç‚¹çŠ¶æ€
curl http://localhost:8000/api/v1/crawlers/sites/bbc/status

# æŸ¥çœ‹ç‰¹å®šç«™ç‚¹çš„æœ€è¿‘è¿è¡Œè®°å½•
curl http://localhost:8000/api/v1/crawlers/sites/bbc
```

## ğŸ” ç›‘æ§ä»»åŠ¡æ‰§è¡Œ

### æŸ¥çœ‹ä»»åŠ¡æ—¥å¿—

ä»»åŠ¡æ‰§è¡Œæ—¥å¿—ä¼šè¾“å‡ºåˆ°ï¼š
- åç«¯æœåŠ¡æ§åˆ¶å°ï¼ˆå¦‚æœä½¿ç”¨ `uvicorn` å¯åŠ¨ï¼‰
- Prefect WebUI çš„ Flow Run è¯¦æƒ…é¡µé¢
- æ—¥å¿—æ–‡ä»¶ï¼ˆå¦‚æœé…ç½®äº†æ–‡ä»¶æ—¥å¿—ï¼‰

### åœ¨ Prefect WebUI ä¸­æŸ¥çœ‹æ—¥å¿—

1. è®¿é—® http://localhost:4200
2. è¿›å…¥ **"Flow Runs"** é¡µé¢
3. ç‚¹å‡»è¦æŸ¥çœ‹çš„ä»»åŠ¡
4. åœ¨è¯¦æƒ…é¡µé¢æŸ¥çœ‹å®æ—¶æ—¥å¿—

## âš™ï¸ ä¸²è¡Œ vs å¹¶è¡Œ

### ä¸²è¡Œæ‰§è¡Œï¼ˆé»˜è®¤ï¼‰

- ä»»åŠ¡æŒ‰é¡ºåºé€ä¸ªè§¦å‘
- é€‚åˆï¼šéœ€è¦æ§åˆ¶èµ„æºä½¿ç”¨ã€é¿å…è¿‡è½½
- ä½¿ç”¨ï¼šä¸æ·»åŠ  `--parallel` å‚æ•°

```bash
python scripts/trigger_crawlers.py --sites bbc hackernews techcrunch
```

### å¹¶è¡Œæ‰§è¡Œ

- æ‰€æœ‰ä»»åŠ¡åŒæ—¶è§¦å‘
- é€‚åˆï¼šéœ€è¦å¿«é€Ÿå¯åŠ¨æ‰€æœ‰ä»»åŠ¡
- ä½¿ç”¨ï¼šæ·»åŠ  `--parallel` å‚æ•°

```bash
python scripts/trigger_crawlers.py --sites bbc hackernews techcrunch --parallel
```

**æ³¨æ„**: å¹¶è¡Œæ‰§è¡Œä¼šåŒæ—¶å¯åŠ¨å¤šä¸ªä»»åŠ¡ï¼Œç¡®ä¿ï¼š
- Prefect Worker æœ‰è¶³å¤Ÿçš„å¹¶å‘èƒ½åŠ›
- ç³»ç»Ÿèµ„æºï¼ˆCPUã€å†…å­˜ã€ç½‘ç»œï¼‰å……è¶³
- ç›®æ ‡ç½‘ç«™å¯ä»¥æ‰¿å—å¹¶å‘è¯·æ±‚

## ğŸ› æ•…éšœæ’é™¤

### é—®é¢˜ 1: ä»»åŠ¡æœªå¯åŠ¨

**æ£€æŸ¥**:
1. åç«¯æœåŠ¡æ˜¯å¦è¿è¡Œ: `curl http://localhost:8000/health`
2. Prefect æœåŠ¡å™¨æ˜¯å¦è¿è¡Œ: `docker compose ps prefect-server`
3. Prefect Worker æ˜¯å¦è¿è¡Œï¼ˆéœ€è¦ worker æ¥æ‰§è¡Œä»»åŠ¡ï¼‰

**è§£å†³æ–¹æ¡ˆ**:
```bash
# å¯åŠ¨ Prefect Worker
export PREFECT_API_URL="http://localhost:4200/api"
prefect worker start --work-queue default
```

### é—®é¢˜ 2: ç«™ç‚¹ä¸å­˜åœ¨

**é”™è¯¯**: `Site xxx not found`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æŸ¥çœ‹æ‰€æœ‰å¯ç”¨ç«™ç‚¹
curl http://localhost:8000/api/v1/crawlers/sites
```

### é—®é¢˜ 3: API è¿æ¥å¤±è´¥

**é”™è¯¯**: `Connection refused` æˆ– `Connection timeout`

**è§£å†³æ–¹æ¡ˆ**:
1. ç¡®ä¿åç«¯æœåŠ¡æ­£åœ¨è¿è¡Œ
2. æ£€æŸ¥ API URL æ˜¯å¦æ­£ç¡®
3. ä½¿ç”¨ `--api-url` å‚æ•°æŒ‡å®šæ­£ç¡®çš„åœ°å€

### é—®é¢˜ 4: ä»»åŠ¡æ‰§è¡Œå¤±è´¥

**æ£€æŸ¥**:
1. åœ¨ Prefect WebUI ä¸­æŸ¥çœ‹ä»»åŠ¡æ—¥å¿—
2. æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼ˆæ˜¯å¦èƒ½è®¿é—®ç›®æ ‡ç½‘ç«™ï¼‰
3. æ£€æŸ¥ç«™ç‚¹é…ç½®æ˜¯å¦æ­£ç¡®

## ğŸ“ API å‚è€ƒ

### POST /api/v1/crawlers/sites/{site_name}/crawl

è§¦å‘å•ä¸ªç«™ç‚¹çš„çˆ¬è™«ä»»åŠ¡ã€‚

**å“åº”ç¤ºä¾‹**:
```json
{
  "message": "Crawl task started for bbc",
  "site": "bbc",
  "flow_run_id": "abc123-def456-ghi789"
}
```

### POST /api/v1/crawlers/sites/batch-crawl

æ‰¹é‡è§¦å‘å¤šä¸ªç«™ç‚¹çš„çˆ¬è™«ä»»åŠ¡ã€‚

**è¯·æ±‚ä½“**:
```json
{
  "sites": ["bbc", "hackernews"],  // null è¡¨ç¤ºæ‰€æœ‰ç«™ç‚¹
  "parallel": true  // æ˜¯å¦å¹¶è¡Œæ‰§è¡Œ
}
```

**å“åº”ç¤ºä¾‹**:
```json
{
  "message": "Triggered 2 crawl tasks",
  "total": 2,
  "success": 2,
  "failed": 0,
  "results": {
    "bbc": {
      "success": true,
      "site": "bbc",
      "flow_run_id": "abc123",
      "message": "Crawl task started for bbc"
    },
    "hackernews": {
      "success": true,
      "site": "hackernews",
      "flow_run_id": "def456",
      "message": "Crawl task started for hackernews"
    }
  }
}
```

## ğŸ¯ æœ€ä½³å®è·µ

1. **é¦–æ¬¡è¿è¡Œ**: å…ˆè§¦å‘å•ä¸ªç«™ç‚¹æµ‹è¯•ï¼Œç¡®ä¿é…ç½®æ­£ç¡®
2. **æ‰¹é‡è¿è¡Œ**: ä½¿ç”¨å¹¶è¡Œæ¨¡å¼å¿«é€Ÿå¯åŠ¨æ‰€æœ‰ä»»åŠ¡
3. **ç›‘æ§æ‰§è¡Œ**: åœ¨ Prefect WebUI ä¸­ç›‘æ§ä»»åŠ¡çŠ¶æ€
4. **é”™è¯¯å¤„ç†**: æ£€æŸ¥å¤±è´¥çš„ä»»åŠ¡æ—¥å¿—ï¼Œä¿®å¤é—®é¢˜åé‡æ–°è§¦å‘
5. **èµ„æºç®¡ç†**: æ ¹æ®ç³»ç»Ÿèµ„æºè°ƒæ•´å¹¶å‘æ•°é‡

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [å¯åŠ¨æŒ‡å—](./STARTUP_GUIDE.md) - å¦‚ä½•å¯åŠ¨åç«¯æœåŠ¡
- [Prefect WebUI æŒ‡å—](./PREFECT_WEBUI_GUIDE.md) - å¦‚ä½•æŸ¥çœ‹å’Œç®¡ç†ä»»åŠ¡

